{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-16T06:48:26.712554Z",
     "iopub.status.busy": "2025-05-16T06:48:26.711950Z",
     "iopub.status.idle": "2025-05-16T06:48:26.718110Z",
     "shell.execute_reply": "2025-05-16T06:48:26.717401Z",
     "shell.execute_reply.started": "2025-05-16T06:48:26.712531Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Tuple\n",
    "from PIL import Image\n",
    "from einops import rearrange\n",
    "from torchmetrics import Metric\n",
    "import editdistance\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T06:48:26.719493Z",
     "iopub.status.busy": "2025-05-16T06:48:26.719300Z",
     "iopub.status.idle": "2025-05-16T06:48:26.742053Z",
     "shell.execute_reply": "2025-05-16T06:48:26.741438Z",
     "shell.execute_reply.started": "2025-05-16T06:48:26.719478Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CROHMEVocab:\n",
    "    PAD_IDX = 0\n",
    "    SOS_IDX = 1\n",
    "    EOS_IDX = 2\n",
    "\n",
    "    def __init__(self, dict_path: str = \"/kaggle/input/crohme/dictionary.txt\") -> None:\n",
    "        self.word2idx = {\"<pad>\": self.PAD_IDX, \"<sos>\": self.SOS_IDX, \"<eos>\": self.EOS_IDX}\n",
    "        with open(dict_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                w = line.strip()\n",
    "                self.word2idx[w] = len(self.word2idx)\n",
    "        self.idx2word: Dict[int, str] = {v: k for k, v in self.word2idx.items()}\n",
    "\n",
    "    def words2indices(self, words: List[str]) -> List[int]:\n",
    "        return [self.word2idx[w] for w in words]\n",
    "\n",
    "    def indices2words(self, id_list: List[int]) -> List[str]:\n",
    "        return [self.idx2word[i] for i in id_list]\n",
    "\n",
    "    def indices2label(self, id_list: List[int]) -> str:\n",
    "        return \" \".join(self.indices2words(id_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T06:48:26.743024Z",
     "iopub.status.busy": "2025-05-16T06:48:26.742806Z",
     "iopub.status.idle": "2025-05-16T06:48:26.760355Z",
     "shell.execute_reply": "2025-05-16T06:48:26.759622Z",
     "shell.execute_reply.started": "2025-05-16T06:48:26.742999Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CROHMEDataset(Dataset):\n",
    "    def __init__(self, root_dir: str):\n",
    "        self.img_dir = os.path.join(root_dir, \"img\")\n",
    "        caption_path = os.path.join(root_dir, \"caption.txt\")\n",
    "        self.data = self._load_captions(caption_path)\n",
    "        self.vocab = CROHMEVocab()\n",
    "        self.to_tensor = ToTensor()\n",
    "\n",
    "    def _load_captions(self, caption_path: str) -> List[Tuple[str, List[str]]]:\n",
    "        with open(caption_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return [ (line.strip().split()[0], line.strip().split()[1:]) for line in f ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name, formula = self.data[idx]\n",
    "        img_path = os.path.join(self.img_dir, f\"{img_name}.bmp\")\n",
    "        image = Image.open(img_path).convert(\"L\")\n",
    "        image_tensor = self.to_tensor(image)\n",
    "        formula_indices = self.vocab.words2indices(formula)\n",
    "        return img_name, image_tensor, formula_indices\n",
    "\n",
    "def collate_fn(batch):\n",
    "    fnames, images, formulas = zip(*batch)\n",
    "    heights = [img.shape[1] for img in images]\n",
    "    widths = [img.shape[2] for img in images]\n",
    "    max_height, max_width = max(heights), max(widths)\n",
    "\n",
    "    batch_size = len(images)\n",
    "    imgs = torch.zeros(batch_size, 1, max_height, max_width)\n",
    "    masks = torch.ones(batch_size, max_height, max_width, dtype=torch.bool)\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        h, w = img.shape[1:]\n",
    "        imgs[i, :, :h, :w] = img\n",
    "        masks[i, :h, :w] = 0\n",
    "\n",
    "    return fnames, imgs, masks, formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T06:48:26.761972Z",
     "iopub.status.busy": "2025-05-16T06:48:26.761750Z",
     "iopub.status.idle": "2025-05-16T06:48:26.778904Z",
     "shell.execute_reply": "2025-05-16T06:48:26.778241Z",
     "shell.execute_reply.started": "2025-05-16T06:48:26.761954Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BTTR(nn.Module):\n",
    "    def __init__(self, d_model, growth_rate, num_layers, nhead, num_decoder_layers, dim_feedforward, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(d_model, growth_rate, num_layers)\n",
    "        self.decoder = Decoder(d_model, nhead, num_decoder_layers, dim_feedforward, dropout)\n",
    "\n",
    "    def forward(self, img, img_mask, tgt):\n",
    "        features, mask = self.encoder(img, img_mask)\n",
    "        features = torch.cat([features, features], dim=0)\n",
    "        mask = torch.cat([mask, mask], dim=0)\n",
    "        return self.decoder(features, mask, tgt)\n",
    "\n",
    "    def beam_search(self, img, img_mask, beam_size, max_len):\n",
    "        features, mask = self.encoder(img, img_mask)\n",
    "        return self.decoder.beam_search(features, mask, beam_size, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T06:48:26.779809Z",
     "iopub.status.busy": "2025-05-16T06:48:26.779560Z",
     "iopub.status.idle": "2025-05-16T06:48:26.801518Z",
     "shell.execute_reply": "2025-05-16T06:48:26.800757Z",
     "shell.execute_reply.started": "2025-05-16T06:48:26.779793Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class WordPosEnc(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500, temperature=10000.0):\n",
    "        super().__init__()\n",
    "        pos = torch.arange(0, max_len).float()\n",
    "        dim_t = torch.arange(0, d_model, 2).float()\n",
    "        div_term = 1.0 / (temperature ** (dim_t / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(pos[:, None] * div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos[:, None] * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, seq_len, _ = x.size()\n",
    "        return x + self.pe[:seq_len, :].unsqueeze(0)\n",
    "\n",
    "class ImgPosEnc(nn.Module):\n",
    "    def __init__(self, d_model, temperature=10000.0, normalize=False):\n",
    "        super().__init__()\n",
    "        assert d_model % 4 == 0, \"d_model must be divisible by 4 for 2D encoding\"\n",
    "        self.half_d_model = d_model // 2\n",
    "        self.temperature = temperature\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        not_mask = ~mask\n",
    "        y_embed = not_mask.cumsum(1, dtype=torch.float32)\n",
    "        x_embed = not_mask.cumsum(2, dtype=torch.float32)\n",
    "\n",
    "        if self.normalize:\n",
    "            eps = 1e-6\n",
    "            y_embed = y_embed / (y_embed[:, -1:, :] + eps) * 2 * math.pi\n",
    "            x_embed = x_embed / (x_embed[:, :, -1:] + eps) * 2 * math.pi\n",
    "\n",
    "        dim_t = torch.arange(self.half_d_model // 2, dtype=torch.float32, device=x.device)\n",
    "        inv_freq = 1.0 / (self.temperature ** (dim_t / (self.half_d_model // 2)))\n",
    "\n",
    "        pos_x = torch.einsum('b h w, d -> b h w d', x_embed, inv_freq)\n",
    "        pos_y = torch.einsum('b h w, d -> b h w d', y_embed, inv_freq)\n",
    "\n",
    "        pos_x = torch.cat([pos_x.sin(), pos_x.cos()], dim=-1)\n",
    "        pos_y = torch.cat([pos_y.sin(), pos_y.cos()], dim=-1)\n",
    "        pos = torch.cat([pos_x, pos_y], dim=-1)  # final shape: [b, h, w, d_model]\n",
    "\n",
    "        assert pos.shape[-1] == x.shape[-1], f\"PosEnc shape mismatch: {pos.shape[-1]} != {x.shape[-1]}\"\n",
    "        return x + pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T06:48:26.839713Z",
     "iopub.status.busy": "2025-05-16T06:48:26.839176Z",
     "iopub.status.idle": "2025-05-16T06:48:26.856842Z",
     "shell.execute_reply": "2025-05-16T06:48:26.855966Z",
     "shell.execute_reply.started": "2025-05-16T06:48:26.839690Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class _Bottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, growth_rate, use_dropout):\n",
    "        super().__init__()\n",
    "        inter_ch = 4 * growth_rate\n",
    "        self.conv1 = nn.Conv2d(in_ch, inter_ch, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(inter_ch)\n",
    "        self.conv2 = nn.Conv2d(inter_ch, growth_rate, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(growth_rate)\n",
    "        self.dropout = nn.Dropout(0.2) if use_dropout else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.dropout(out)\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.dropout(out)\n",
    "        return torch.cat([x, out], dim=1)\n",
    "\n",
    "class _Transition(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, use_dropout):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_ch)\n",
    "        self.dropout = nn.Dropout(0.2) if use_dropout else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn(self.conv(x)))\n",
    "        out = self.dropout(out)\n",
    "        out = F.avg_pool2d(out, 2, ceil_mode=True)\n",
    "        return out\n",
    "    \n",
    "class _SingleLayer(nn.Module):\n",
    "    def __init__(self, in_ch, growth_rate, use_dropout):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, growth_rate, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(growth_rate)\n",
    "        self.dropout = nn.Dropout(0.2) if use_dropout else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn(self.conv(x)))\n",
    "        out = self.dropout(out)\n",
    "        return torch.cat([x, out], dim=1)\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growth_rate, num_layers, reduction=0.5, bottleneck=True, use_dropout=True):\n",
    "        super().__init__()\n",
    "        n_ch = 2 * growth_rate\n",
    "        self.conv1 = nn.Conv2d(1, n_ch, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.norm1 = nn.BatchNorm2d(n_ch)\n",
    "\n",
    "        self.dense1 = self._make_dense(n_ch, growth_rate, num_layers, bottleneck, use_dropout)\n",
    "        n_ch += num_layers * growth_rate\n",
    "        out_ch = int(n_ch * reduction)\n",
    "        self.trans1 = _Transition(n_ch, out_ch, use_dropout)\n",
    "\n",
    "        n_ch = out_ch\n",
    "        self.dense2 = self._make_dense(n_ch, growth_rate, num_layers, bottleneck, use_dropout)\n",
    "        n_ch += num_layers * growth_rate\n",
    "        out_ch = int(n_ch * reduction)\n",
    "        self.trans2 = _Transition(n_ch, out_ch, use_dropout)\n",
    "\n",
    "        n_ch = out_ch\n",
    "        self.dense3 = self._make_dense(n_ch, growth_rate, num_layers, bottleneck, use_dropout)\n",
    "        n_ch += num_layers * growth_rate\n",
    "\n",
    "        self.post_norm = nn.BatchNorm2d(n_ch)\n",
    "        self.out_channels = n_ch\n",
    "\n",
    "    def _make_dense(self, in_ch, growth_rate, num_layers, bottleneck, use_dropout):\n",
    "        layers = []\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(_Bottleneck(in_ch, growth_rate, use_dropout) if bottleneck else _SingleLayer(in_ch, growth_rate, use_dropout))\n",
    "            in_ch += growth_rate\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        out = self.conv1(x)\n",
    "        out = self.norm1(out)\n",
    "        out_mask = mask[:, ::2, ::2]\n",
    "\n",
    "        out = F.relu(out)\n",
    "        out = F.max_pool2d(out, 2, ceil_mode=True)\n",
    "        out_mask = out_mask[:, ::2, ::2]\n",
    "\n",
    "        out = self.trans1(self.dense1(out))\n",
    "        out_mask = out_mask[:, ::2, ::2]\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        out_mask = out_mask[:, ::2, ::2]\n",
    "        out = self.dense3(out)\n",
    "        out = self.post_norm(out)\n",
    "        return out, out_mask\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, growth_rate, num_layers):\n",
    "        super().__init__()\n",
    "        self.densenet = DenseNet(growth_rate, num_layers)\n",
    "        self.feature_proj = nn.Conv2d(self.densenet.out_channels, d_model, kernel_size=1)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.pos_enc = ImgPosEnc(d_model, normalize=True)\n",
    "\n",
    "    def forward(self, img, mask):\n",
    "        features, mask = self.densenet(img, mask)\n",
    "        features = self.feature_proj(features)\n",
    "        features = rearrange(features, 'b d h w -> b h w d')\n",
    "        features = self.norm(features)\n",
    "        features = self.pos_enc(features, mask)\n",
    "        features = rearrange(features, 'b h w d -> b (h w) d')\n",
    "        mask = rearrange(mask, 'b h w -> b (h w)')\n",
    "        return features, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T06:48:26.899254Z",
     "iopub.status.busy": "2025-05-16T06:48:26.898930Z",
     "iopub.status.idle": "2025-05-16T06:48:26.917729Z",
     "shell.execute_reply": "2025-05-16T06:48:26.917157Z",
     "shell.execute_reply.started": "2025-05-16T06:48:26.899230Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Hypothesis:\n",
    "    def __init__(self, seq_tensor: torch.Tensor, score: float, direction: str):\n",
    "        assert direction in {\"l2r\", \"r2l\"}\n",
    "        raw_seq = seq_tensor.tolist()\n",
    "        self.seq = raw_seq[::-1] if direction == \"r2l\" else raw_seq\n",
    "        self.score = score\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq) if self.seq else 1\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"seq: {self.seq}, score: {self.score}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T06:48:26.858469Z",
     "iopub.status.busy": "2025-05-16T06:48:26.858278Z",
     "iopub.status.idle": "2025-05-16T06:48:26.878720Z",
     "shell.execute_reply": "2025-05-16T06:48:26.877991Z",
     "shell.execute_reply.started": "2025-05-16T06:48:26.858453Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_decoder_layers, dim_feedforward, dropout):\n",
    "        super().__init__()\n",
    "        self.word_embed = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, d_model),\n",
    "            nn.LayerNorm(d_model)\n",
    "        )\n",
    "        self.pos_enc = WordPosEnc(d_model)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
    "        self.transformer = nn.TransformerDecoder(decoder_layer, num_decoder_layers)\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src, src_mask, tgt):\n",
    "        b, l = tgt.size()\n",
    "        tgt_mask = torch.triu(torch.ones(l, l, device=tgt.device), diagonal=1).bool()\n",
    "        tgt_pad_mask = tgt == vocab.PAD_IDX\n",
    "\n",
    "        tgt_emb = self.word_embed(tgt)\n",
    "        tgt_emb = self.pos_enc(tgt_emb)\n",
    "\n",
    "        src = rearrange(src, 'b s d -> s b d')\n",
    "        tgt_emb = rearrange(tgt_emb, 'b l d -> l b d')\n",
    "\n",
    "        out = self.transformer(tgt_emb, src, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_pad_mask, memory_key_padding_mask=src_mask)\n",
    "        out = rearrange(out, 'l b d -> b l d')\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "    def beam_search(self, src, mask, beam_size, max_len):\n",
    "        assert src.size(0) == 1, \"beam_search expects batch size 1\"\n",
    "\n",
    "        start_token = vocab.SOS_IDX\n",
    "        stop_token = vocab.EOS_IDX\n",
    "\n",
    "        hypotheses = torch.full((1, max_len + 1), vocab.PAD_IDX, dtype=torch.long, device=src.device)\n",
    "        hypotheses[:, 0] = start_token\n",
    "        hyp_scores = torch.zeros(1, device=src.device)\n",
    "        completed_hyps = []\n",
    "\n",
    "        t = 0\n",
    "        while len(completed_hyps) < beam_size and t < max_len:\n",
    "            hyp_num = hypotheses.size(0)\n",
    "            exp_src = src.expand(hyp_num, -1, -1)\n",
    "            exp_mask = mask.expand(hyp_num, -1)\n",
    "\n",
    "            out = self.forward(exp_src, exp_mask, hypotheses)  # logits: [b, l, vocab_size]\n",
    "            log_probs = torch.log_softmax(out[:, t, :], dim=-1)  # [b, vocab_size]\n",
    "\n",
    "            new_hyp_scores = hyp_scores.unsqueeze(1) + log_probs  # [b, vocab_size]\n",
    "            flat_scores = new_hyp_scores.view(-1)\n",
    "            top_scores, top_indices = flat_scores.topk(beam_size - len(completed_hyps))\n",
    "\n",
    "            prev_hyp_ids = top_indices // log_probs.size(1)\n",
    "            next_token_ids = top_indices % log_probs.size(1)\n",
    "\n",
    "            new_hypotheses = []\n",
    "            new_hyp_scores = []\n",
    "\n",
    "            for prev_hyp_id, next_token_id, score in zip(prev_hyp_ids, next_token_ids, top_scores):\n",
    "                next_token_id = next_token_id.item()\n",
    "                score = score.item()\n",
    "\n",
    "                new_hyp = hypotheses[prev_hyp_id].clone()\n",
    "                new_hyp[t + 1] = next_token_id\n",
    "\n",
    "                if next_token_id == stop_token:\n",
    "                    completed_hyps.append(Hypothesis(new_hyp[1:t+1], score, direction=\"l2r\"))\n",
    "                else:\n",
    "                    new_hypotheses.append(new_hyp)\n",
    "                    new_hyp_scores.append(score)\n",
    "\n",
    "            if len(new_hypotheses) == 0:\n",
    "                break\n",
    "\n",
    "            hypotheses = torch.stack(new_hypotheses, dim=0)\n",
    "            hyp_scores = torch.tensor(new_hyp_scores, device=src.device)\n",
    "\n",
    "            t += 1\n",
    "\n",
    "        if len(completed_hyps) == 0:\n",
    "            completed_hyps.append(Hypothesis(hypotheses[0][1:], hyp_scores[0].item(), direction=\"l2r\"))\n",
    "\n",
    "        return completed_hyps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T06:48:26.879778Z",
     "iopub.status.busy": "2025-05-16T06:48:26.879509Z",
     "iopub.status.idle": "2025-05-16T06:48:26.898160Z",
     "shell.execute_reply": "2025-05-16T06:48:26.897630Z",
     "shell.execute_reply.started": "2025-05-16T06:48:26.879756Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def beam_search_batch(model, imgs, masks, beam_size, max_len, alpha, vocab):\n",
    "    batch_size = imgs.size(0)\n",
    "    results = []\n",
    "    for i in range(batch_size):\n",
    "        img, mask = imgs[i].unsqueeze(0), masks[i].unsqueeze(0)\n",
    "        hyps = model.beam_search(img, mask, beam_size, max_len)\n",
    "        best = max(hyps, key=lambda h: h.score / (len(h) ** alpha))\n",
    "        results.append(best.seq)\n",
    "    return results\n",
    "\n",
    "def ensemble_beam_search_batch(models, imgs, masks, beam_size, max_len, alpha, vocab):\n",
    "    batch_size = imgs.size(0)\n",
    "    results = []\n",
    "    for i in range(batch_size):\n",
    "        img, mask = imgs[i].unsqueeze(0), masks[i].unsqueeze(0)\n",
    "        all_hyps = []\n",
    "        for model in models:\n",
    "            hyps = model.beam_search(img, mask, beam_size, max_len)\n",
    "            all_hyps.extend(hyps)\n",
    "        best = max(all_hyps, key=lambda h: h.score / (len(h) ** alpha))\n",
    "        results.append(best.seq)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T06:48:26.919335Z",
     "iopub.status.busy": "2025-05-16T06:48:26.919148Z",
     "iopub.status.idle": "2025-05-16T06:48:26.934736Z",
     "shell.execute_reply": "2025-05-16T06:48:26.933993Z",
     "shell.execute_reply.started": "2025-05-16T06:48:26.919320Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def ce_loss(output_hat: torch.Tensor, output: torch.Tensor) -> torch.Tensor:\n",
    "    flat_hat = rearrange(output_hat, \"b l e -> (b l) e\")\n",
    "    flat = rearrange(output, \"b l -> (b l)\")\n",
    "    return F.cross_entropy(flat_hat, flat, ignore_index=vocab.PAD_IDX)\n",
    "\n",
    "def to_tgt_output(tokens, direction, device):\n",
    "    assert direction in {\"l2r\", \"r2l\"}\n",
    "\n",
    "    tokens = [torch.tensor(t, dtype=torch.long) for t in tokens]\n",
    "    if direction == \"l2r\":\n",
    "        start_w, stop_w = vocab.SOS_IDX, vocab.EOS_IDX\n",
    "    else:\n",
    "        tokens = [torch.flip(t, dims=[0]) for t in tokens]\n",
    "        start_w, stop_w = vocab.EOS_IDX, vocab.SOS_IDX\n",
    "\n",
    "    batch_size = len(tokens)\n",
    "    max_len = max(len(t) for t in tokens)\n",
    "    tgt = torch.full((batch_size, max_len + 1), vocab.PAD_IDX, dtype=torch.long, device=device)\n",
    "    out = torch.full((batch_size, max_len + 1), vocab.PAD_IDX, dtype=torch.long, device=device)\n",
    "\n",
    "    for i, t in enumerate(tokens):\n",
    "        tgt[i, 0] = start_w\n",
    "        tgt[i, 1:1+len(t)] = t\n",
    "        out[i, :len(t)] = t\n",
    "        out[i, len(t)] = stop_w\n",
    "\n",
    "    return tgt, out\n",
    "\n",
    "def to_bi_tgt_out(tokens, device):\n",
    "    l2r_tgt, l2r_out = to_tgt_output(tokens, \"l2r\", device)\n",
    "    r2l_tgt, r2l_out = to_tgt_output(tokens, \"r2l\", device)\n",
    "    tgt = torch.cat((l2r_tgt, r2l_tgt), dim=0)\n",
    "    out = torch.cat((l2r_out, r2l_out), dim=0)\n",
    "    return tgt, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T06:48:26.936081Z",
     "iopub.status.busy": "2025-05-16T06:48:26.935463Z",
     "iopub.status.idle": "2025-05-16T06:48:26.955077Z",
     "shell.execute_reply": "2025-05-16T06:48:26.954392Z",
     "shell.execute_reply.started": "2025-05-16T06:48:26.936059Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ExpRateRecorder(Metric):\n",
    "    def __init__(self, dist_sync_on_step=False):\n",
    "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
    "        self.add_state(\"total\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"correct\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, indices_hat, indices):\n",
    "        dist = editdistance.eval(indices_hat, indices)\n",
    "        if dist == 0:\n",
    "            self.correct += 1\n",
    "        self.total += 1\n",
    "\n",
    "    def compute(self):\n",
    "        return (self.correct / self.total).item() if self.total > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T06:48:26.955867Z",
     "iopub.status.busy": "2025-05-16T06:48:26.955703Z",
     "iopub.status.idle": "2025-05-16T06:48:33.675247Z",
     "shell.execute_reply": "2025-05-16T06:48:33.674644Z",
     "shell.execute_reply.started": "2025-05-16T06:48:26.955853Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">earthy-fire-1</strong> at: <a href='https://wandb.ai/trung-nm2109/BTTR_offline_hmer/runs/yrck5po5' target=\"_blank\">https://wandb.ai/trung-nm2109/BTTR_offline_hmer/runs/yrck5po5</a><br> View project at: <a href='https://wandb.ai/trung-nm2109/BTTR_offline_hmer' target=\"_blank\">https://wandb.ai/trung-nm2109/BTTR_offline_hmer</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250516_064251-yrck5po5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250516_064826-r3law0l6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/trung-nm2109/BTTR_offline_hmer/runs/r3law0l6' target=\"_blank\">copper-oath-2</a></strong> to <a href='https://wandb.ai/trung-nm2109/BTTR_offline_hmer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/trung-nm2109/BTTR_offline_hmer' target=\"_blank\">https://wandb.ai/trung-nm2109/BTTR_offline_hmer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/trung-nm2109/BTTR_offline_hmer/runs/r3law0l6' target=\"_blank\">https://wandb.ai/trung-nm2109/BTTR_offline_hmer/runs/r3law0l6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/trung-nm2109/BTTR_offline_hmer/runs/r3law0l6?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7ad287bd1710>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = CROHMEVocab()\n",
    "vocab_size = len(vocab)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "wandb.login(\n",
    "    key = \"9bce7f20a794219664f78217ccc84b283dbb0cea\",\n",
    ")\n",
    "wandb.init(\n",
    "    project = \"BTTR_offline_hmer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T06:48:33.676364Z",
     "iopub.status.busy": "2025-05-16T06:48:33.676094Z",
     "iopub.status.idle": "2025-05-16T06:49:53.149261Z",
     "shell.execute_reply": "2025-05-16T06:49:53.148205Z",
     "shell.execute_reply.started": "2025-05-16T06:48:33.676339Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 305/305 [01:17<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 3.6521\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "53",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35/2800620203.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;31m# if __name__ == \"__main__\":\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;31m#     main()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_35/2800620203.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;31m# Optional: Add beam search for validation exprate if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeam_search_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0mgt_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords2indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformulas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                 \u001b[0mexp_rate_recoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_35/3674459364.py\u001b[0m in \u001b[0;36mwords2indices\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords2indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mindices2words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_list\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_35/3674459364.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords2indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mindices2words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_list\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 53"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    # Configs\n",
    "    best_val_loss = float('inf')\n",
    "    seed = 1337\n",
    "    train_root = \"/kaggle/input/crohme/resources/resources/CROHME/train\"\n",
    "    val_root = \"/kaggle/input/crohme/resources/resources/CROHME/val\"\n",
    "    batch_size = 32\n",
    "    num_epochs = 100\n",
    "    learning_rate = 0.01\n",
    "    patience = 10\n",
    "    checkpoint_dir = \"/kaggle/working/checkpoints\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    # Data Loaders\n",
    "    train_dataset = CROHMEDataset(train_root)\n",
    "    val_dataset = CROHMEDataset(val_root)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Model\n",
    "    model = BTTR(\n",
    "        d_model=256,\n",
    "        growth_rate=16,\n",
    "        num_layers=3,\n",
    "        nhead=8,\n",
    "        num_decoder_layers=3,\n",
    "        dim_feedforward=1024,\n",
    "        dropout=0.1\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=learning_rate, eps=1e-6, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.1, patience=patience)\n",
    "\n",
    "    exp_rate_recorder = ExpRateRecorder()\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for fnames, imgs, masks, formulas in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "            imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n",
    "            tgt, out = to_bi_tgt_out(formulas, DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out_hat = model(imgs, masks, tgt)\n",
    "            loss = ce_loss(out_hat, out)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        exp_rate_recorder.reset()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for fnames, imgs, masks, formulas in val_loader:\n",
    "                imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n",
    "                tgt, out = to_bi_tgt_out(formulas, DEVICE)\n",
    "\n",
    "                out_hat = model(imgs, masks, tgt)\n",
    "                loss = ce_loss(out_hat, out)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Optional: Add beam search for validation exprate if needed\n",
    "                preds = beam_search_batch(model, imgs, masks, beam_size=10, max_len=200, alpha=1.0, vocab=vocab)\n",
    "                gt_indices = vocab.words2indices(formulas[0])\n",
    "                exp_rate_recorder.update(preds[0], gt_indices)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_exprate = exp_rate_recorder.compute()\n",
    "        print(f\"Epoch {epoch}: Val Loss = {avg_val_loss:.4f} | Val Exprate = {val_exprate:.4f}\")\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Scheduler step based on ExpRate (set to dummy for now)\n",
    "        scheduler.step(val_exprate)\n",
    "\n",
    "        # wandb.log({\n",
    "        #     \"epoch\": epoch,\n",
    "        #     \"train_loss\": avg_train_loss,\n",
    "        #     \"val_loss\": avg_val_loss\n",
    "        # })\n",
    "        wandb.log({\"Epoch\": epoch, \"Train loss\": avg_train_loss, \"Valid loss\": avg_val_loss, \"Val exprate\": val_exprate})\n",
    "\n",
    "        # Save checkpoint\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'vocab': vocab\n",
    "            }\n",
    "            torch.save(checkpoint, os.path.join(checkpoint_dir, 'bttr_best.pth'))\n",
    "            print('model saved!')\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-16T06:49:53.149936Z",
     "iopub.status.idle": "2025-05-16T06:49:53.150252Z",
     "shell.execute_reply": "2025-05-16T06:49:53.150108Z",
     "shell.execute_reply.started": "2025-05-16T06:49:53.150093Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# def main():\n",
    "#     # Config\n",
    "#     checkpoint_path = \"checkpoints/epoch_50.pth\"\n",
    "#     data_root = \"resources/CROHME/test\"\n",
    "#     batch_size = 4  # batch-wise testing for speed\n",
    "\n",
    "#     # Data\n",
    "#     vocab = CROHMEVocab()\n",
    "#     test_dataset = CROHMEDataset(data_root)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "#     # Model\n",
    "#     model = BTTR(\n",
    "#         d_model=256,\n",
    "#         growth_rate=16,\n",
    "#         num_layers=3,\n",
    "#         nhead=8,\n",
    "#         num_decoder_layers=3,\n",
    "#         dim_feedforward=1024,\n",
    "#         dropout=0.1\n",
    "#     ).to(DEVICE)\n",
    "\n",
    "#     state_dict = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "#     model.load_state_dict(state_dict)\n",
    "#     model.eval()\n",
    "\n",
    "#     # Metrics\n",
    "#     recorder = ExpRateRecorder()\n",
    "\n",
    "#     # Inference Loop\n",
    "#     os.makedirs(\"results\", exist_ok=True)\n",
    "#     with torch.no_grad():\n",
    "#         for fnames, imgs, masks, formulas in tqdm(test_loader, desc=\"Testing\"):\n",
    "#             imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n",
    "\n",
    "#             hypotheses_batch = beam_search_batch(model, imgs, masks, beam_size=10, max_len=200, alpha=1.0, vocab=vocab)\n",
    "\n",
    "#             for fname, hyp, gt_formula in zip(fnames, hypotheses_batch, formulas):\n",
    "#                 pred_latex = vocab.indices2label(hyp)\n",
    "#                 gt_indices = vocab.words2indices(gt_formula)\n",
    "\n",
    "#                 recorder.update(hyp, gt_indices)\n",
    "\n",
    "#                 # Save result file\n",
    "#                 with open(f\"results/{fname}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#                     f.write(f\"%{fname}\\n${pred_latex}$\")\n",
    "\n",
    "#     # Final Metrics\n",
    "#     exprate = recorder.compute()\n",
    "#     print(f\"Expression Recognition Rate: {exprate:.4f}\")\n",
    "\n",
    "# # if __name__ == \"__main__\":\n",
    "# #     main()\n",
    "# main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7428146,
     "sourceId": 11824879,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
